Objectives
By completing this hands-on activity, students will:

Enhance Text Processing Skills: Learn to process and analyze text data to extract meaningful insights using Hadoop MapReduce.
Develop Advanced MapReduce Jobs: Implement more complex MapReduce jobs to perform tasks like finding the most frequently used words, calculating dialogue lengths, and extracting unique words by characters.
Deploy and Run a Hadoop Cluster with Docker: Learn how to deploy a Hadoop cluster using Docker and run MapReduce jobs on it.
Submit and Manage Code Using GitHub: Develop skills in managing code and submitting assignments via GitHub.
Setup and Execution
1. Fork the GitHub Repository
First, accept the GitHub Classroom invitation and fork the assignment repository to your own GitHub account.
Once you’ve forked the repo, open the repository in GitHub Codespaces to begin working on the assignment.
2. Start the Hadoop Cluster Using Docker Compose
The repository contains a docker-compose.yml file that configures a Hadoop cluster. Run the following command in the GitHub Codespaces terminal to start the cluster:

docker-compose up -d
This command will spin up the necessary Hadoop components (ResourceManager, NodeManager, etc.) inside Docker containers.

3. Build the Java Code with Maven
After starting the cluster, use Maven to build the Java MapReduce code for the movie script analysis. In the terminal, execute the following command to compile and build the project:

mvn clean install
This command will generate a JAR file in the target/ directory, which contains your MapReduce code.

4. Prepare Input Data Files
The input movie script dialogues dataset is located in the input/ folder of the repository. Ensure that this file (movie_dialogues.txt) is present in the input/ directory.
5. Move the JAR and Input Files to the Docker Container
5.1 Move the JAR File to the Container
Copy the built JAR file to the ResourceManager container. Run this command:

Note: Replace <your-jar-file> with the actual name of the JAR file generated by Maven.

docker cp target/<your-jar-file>.jar resourcemanager:/opt/hadoop-3.2.1/share/hadoop/mapreduce/
5.2 Move the Input File to the Container
Next, copy the movie script dialogues dataset to the ResourceManager container:

docker cp input/movie_dialogues.txt resourcemanager:/opt/hadoop-3.2.1/share/hadoop/mapreduce/
6. Connect to the ResourceManager Container
To run the Hadoop commands, you'll need to connect to the ResourceManager container:

docker exec -it resourcemanager /bin/bash
Once inside the container, navigate to the Hadoop directory where your files were copied:

cd /opt/hadoop-3.2.1/share/hadoop/mapreduce/
7. Set Up HDFS for Input File
To run the MapReduce job, the input file needs to be stored in Hadoop’s distributed file system (HDFS).

7.1 Create Directories in HDFS
Create a directory in HDFS for the input file:

hadoop fs -mkdir -p /input/movie_scripts
7.2 Upload the Input File to HDFS
Upload the movie script dialogues file to HDFS:

hadoop fs -put movie_dialogues.txt /input/movie_scripts/
8. Execute the Movie Script Analysis MapReduce Jobs
Now you are ready to run your MapReduce job for movie script analysis. This job will consist of three tasks:

Most Frequent Words by Character
Dialogue Length Analysis
Unique Words by Character
Run the job using the following command:

Note: Replace <your-jar-file> with the actual name of the JAR file generated by Maven.

hadoop jar <your-jar-file>.jar com.movie.script.analysis.MovieScriptAnalysis /input/movie_scripts/movie_dialogues.txt /output
This command will execute the MapReduce job with the movie script dialogues as the input and store the results in the /output directory in HDFS.

9. View the Output of the MapReduce Job
Note: The output will be stored in multiple directories (one for each task). You can view the output files using the following commands:

9.1 List the Output Directories
hadoop fs -ls /output
9.2 View the Output Files for Each Task
Task 1: Most Frequent Words by Character
hadoop fs -cat /output/task1/part-r-00000
Task 2: Dialogue Length Analysis
hadoop fs -cat /output/task2/part-r-00000
Task 3: Unique Words by Character
hadoop fs -cat /output/task3/part-r-00000
These commands will display the results for each analysis task.

10. Copy Output from HDFS to Local OS
Once you have verified the results, copy the output from HDFS to your local file system.

10.1 Copy Output from HDFS
Use the following command to copy the output from HDFS to the Hadoop directory:

hadoop fs -get /output /opt/hadoop-3.2.1/share/hadoop/mapreduce/
10.2 Copy Output from the Container to Your Local Machine
Now, exit the ResourceManager container:

exit
Next, copy the output files from the Docker container to your GitHub Codespaces environment:

docker cp resourcemanager:/opt/hadoop-3.2.1/share/hadoop/mapreduce/output/ ./output/
11. Submit Your Code and Output
11.1 Push Your Code and Output to GitHub
Commit your changes, including the output from the MapReduce job, and push them to your GitHub repository:

git add .
git commit -m "Completed Movie Script Analysis Assignment"
git push origin main
11.2 Submit the Assignment on GitHub Classroom
Once you've pushed your code, go to GitHub Classroom and ensure your repository is submitted for the assignment. Make sure that the following are included:

The JAR file with your MapReduce job.
The input file (movie_dialogues.txt).
The output files from your MapReduce job.
The one-page report documenting the steps you followed, any challenges faced, and your observations.
